data("Soccer.rda")
load("Soccer.rda")
load("Soccer.rda")
index <- createDataPartition(trainA$Class, p=0.8, list=F)
train <- soccer[index,]
test <- soccer[-index,]
View(soccer)
library(ROCR)
library(caret)
library(randomForest)
library(ggplot2)
load("Soccer.rda")
index <- createDataPartition(soccer$result1, p=0.8, list=F)
train <- soccer[index,]
test <- soccer[-index,]
model_rf1 <- randomForest(result1 ~ ., data = train, ntree = 1000,
nodesize = 50, mtry = 5, importance = T, do.trace = T)
install.packages("AUCRF")
library(AUCRF)
pred1 <- predict(model_rf1, newdata = test)
confusionMatrix(pred, test$result1)
confusionMatrix(pred1, test$result1)
varImpPlot(model_rf1)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 30, mtry = 10, importance = T, do.trace = T)
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
varImpPlot(model_rf2)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 50, mtry = 10, importance = T, do.trace = T)
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
varImpPlot(model_rf2)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 70, mtry = 10, importance = T, do.trace = T)
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
varImpPlot(model_rf2)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 1000,
nodesize = 70, mtry = 10, importance = T, do.trace = T)
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
varImpPlot(model_rf2)
model_rf2$oob.times
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 1000,
nodesize = 70, mtry = 5, importance = T, do.trace = T)
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
varImpPlot(model_rf2)
ncol(train)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 50, mtry = ncol(train), importance = T, do.trace = T)
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
pred1_prob <- predict(model_rf1, newdata = test, type = "prob") #probability prediction for each class
View(pr2)
View(pred1_prob)
confusionMatrix(pred1_prob, test$result1)
pred1_prob
load("Soccer.rda")
set.seed(2015)
soccer <- tuneRF(soccer[, -72], soccer[, 72], stepFactor = 1.5,
trace=T, ntreeTry = 500, improve = 0.1, plot = TRUE, doBest = TRUE)
load("Soccer.rda") # need to run every time for tuning
set.seed(2016)
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 55, mtry = 8, maxnodes = 20, importance = T,
do.trace = T, localImp = T, replace = F)
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
varImpPlot(model_rf2)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 60, mtry = 8, maxnodes = 20, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 60, mtry = 8, maxnodes = 20, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 50, mtry = 8, maxnodes = 30, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 1000,
nodesize = 30, mtry = 8, maxnodes = 20, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 80, mtry = 8, maxnodes = 20, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
pred2 <- predict(model_rf1, newdata = test)
confusionMatrix(pred2, test$result1)
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
set.seed(2016)
model_rf1 <- randomForest(result1 ~ ., data = train, ntree = 1000,
nodesize = 40, mtry = 5, maxnodes = 20, importance = T,
do.trace = T, localImp = T, replace = F)
pred1 <- predict(model_rf1, newdata = test)
pred1_prob <- predict(model_rf1, newdata = test, type = "prob") # predicted probabilities
pred1_prob
confusionMatrix(pred1, test$result1)
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 80, mtry = 8, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 80, mtry = 8, maxnodes = 40, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1)
pred2_prob <- predict(model_rf2, newdata = Test, type="prob")
pred2_prob <- predict(model_rf2, newdata = test, type = "prob")
pred_final <- prediction(pr3[,2], Test$result1, label.ordering = c("HDW", "HW"))
perf_final <- performance(pred_final, "tpr", "fpr")
pred2_prob <- predict(model_rf2, newdata = test, type = "prob")
pred2_prob <- predict(model_rf2, newdata = test, type = "prob")
pred_final <- prediction(pred2_prob[, 2], test$result1, label.ordering = c("HDW", "HW"))
perf_final <- performance(pred_final, "tpr", "fpr")
plot(perf_final)
performance(pred_final, "auc")@y.values #auc= 0.6818087
varImpPlot(model_rf2, sort = T, type = 1, n.var = 40)
confusionMatrix(pred2, test$result1, positive = "HW")
confusionMatrix(pred2, test$result1, positive = "HW")
confusionMatrix(pred1, test$result1, positive = "HW")
# The following dataset consist of data on football games for 11
# european countries.  It covers time from 2011-2016. The variable
# result has two arguments: HDW- Home team didnt win, HW- Homw
# team won. Most of the data describes the players strength
# averaged for the last 7 games (prior to the given game) averaged
# by the team (those are only for players who participated in the
# game).  More information can be found here-
# http://sofifa.com/player/192883.  You can find a huge amount of
# analytics done on football data
# here-https://www.kaggle.com/hugomathien/soccer variables
# 'home_team_points', 'away_team_points' show the amount of the
# points the teams have earned before the game (3 for win, 1 for
# draw, 0 for lose) Variable stage shows the Round number during
# the season. The poitive case for the model is HW (home team
# won).
# Dont forget to set the seed everytime you run randomForest.
# Divide data into training and testing set,80% goes to train.
##################### Packages
install.packages("caret")
library(caret)
install.packages("randomForest")
library(randomForest)
library(e1071)
library(ROCR)
library(corrplot)
library(ggplot2)
library(corrplot)
library(randomForest)
#########################
load("Soccer.rda")
set.seed(2015)
trainindex<-createDataPartition(soccer$result1, p=0.8, list = F )
Train<-soccer[trainindex,]
Test<-soccer[-trainindex,]
# Question 1. Do some descriptive analytics (charts, tests, etc)
# to find interesting patterns in the data (10 points)
summary(soccer) #no NAs
m<-cor(soccer[,1:71])
m
#the data is too big, hence we divide the variables in 2 parts,
#with this we could find multicollinearity
#the darker the colors of the squares the more correlated the variables are
corrplot(m[1:35,1:35], title = "CorMAtrix", bg="white", type="full", mar=c(0,0,0,0), method="square", tl.col = "red", cl.cex = 0.6, cl.ratio = 0.1, cl.align.text = "c")
# for example home potential is mostly correlated with home short passing, home ball control and home reactions variables
#there are only 2 light red colors indicating inverse correlation of almost -0.3 according to the color scale
#between home strenght and home balance
pairs(home_balance~home_strength, data=soccer)#weak ngative
#2nd halve
corrplot(m[36:71,36:71],bg="white", type="full", mar=c(0,0,0,0), method="square", title = "CorMAtrix")
#you can take small ranges for better visability
#for example away vision is the least correlated with away gk reflexes
#and mostly correlated with awal short and long passing and ball control
qplot(away_sliding_tackle, data=soccer, binwidth=0.8) #defending (away-slide-tackling) most frequences at 58.000
#Question 2. Build a random forest model with the package randomForest. Your goal is
# to predict the game result (variable 'result1') (15 points)
set.seed(2015)
model<-randomForest(result1~., data=Train, ntree=500)
# The following dataset consist of data on football games for 11
# european countries.  It covers time from 2011-2016. The variable
# result has two arguments: HDW- Home team didnt win, HW- Homw
# team won. Most of the data describes the players strength
# averaged for the last 7 games (prior to the given game) averaged
# by the team (those are only for players who participated in the
# game).  More information can be found here-
# http://sofifa.com/player/192883.  You can find a huge amount of
# analytics done on football data
# here-https://www.kaggle.com/hugomathien/soccer variables
# 'home_team_points', 'away_team_points' show the amount of the
# points the teams have earned before the game (3 for win, 1 for
# draw, 0 for lose) Variable stage shows the Round number during
# the season. The poitive case for the model is HW (home team
# won).
## libraries that may be needed
library(lattice)
library(ggplot2)
library(e1071)
library(caret)
library(ROCR)
library(caTools)
library(class)
library(randomForest)
library(AUCRF)
# Dont forget to set the seed everytime you run randomForest.
# Divide data into training and testing set,80% goes to train.
load("Soccer.rda")
set.seed(2016)
index <- createDataPartition(soccer$result1, p=0.8, list=F)
train <- soccer[index,]
test <- soccer[-index,]
# Question 1. Do some descriptive analytics (charts, tests, etc)
# to find interesting patterns in the data (10 points)
# Question 2. Build a
# random forest model with the package randomForest. Your goal is
# to predict the game result (variable 'result1') (15 points)
set.seed(2016)
model_rf1 <- randomForest(result1 ~ ., data = train, ntree = 1000,
nodesize = 40, mtry = 5, maxnodes = 20, importance = T,
do.trace = T, localImp = T, replace = F)
pred1 <- predict(model_rf1, newdata = test)
pred1_prob <- predict(model_rf1, newdata = test, type = "prob") # predicted probabilities
pred1_prob
confusionMatrix(pred1, test$result1, positive = "HW") # accuracy = 0.6301
varImpPlot(model_rf1)
# 2.1 Develop randomForest model by tunning several parameters.
# Look for package help for more info.  Explain the meaning of
# each parameter.
## ntree: Number of trees to grow.
## nodesize: This is the minimum node size, it implicitly sets the depth of your trees.
## mtry: Number of variables randomly sampled as candidates at each split.
## replace: being false means that we want model without replacement of cases
## importance: the importance of predictors is being computed
## do.trace: is a stoping criterea in case the error remains the same and the tree grows
set.seed(2016)
mtry_tune <- tuneRF(soccer[, -72], soccer[, 72], stepFactor = 1.5,
trace = TRUE, ntreeTry = 500, improve = 0.1, plot = TRUE, doBest = TRUE)
set.seed(2016)
mtry_tune <- tuneRF(soccer[, -72], soccer[, 72], stepFactor = 1.5,
trace = TRUE, ntreeTry = 500, improve = 1e-5, plot = TRUE, doBest = TRUE)
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 80, mtry = 6, maxnodes = 40, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1, positive = "HW")
set.seed(2016)
mtry_tune <- tuneRF(soccer[, -72], soccer[, 72], stepFactor = 1.5,
trace = TRUE, ntreeTry = 500, improve = 0.1, plot = TRUE, doBest = TRUE)
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 80, mtry = 8, maxnodes = 40, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1, positive = "HW")
varImpPlot(model_rf1, sort = T, type = 1, n.var = 40)
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1, positive = "HW")
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 80, mtry = 8, maxnodes = 40, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1, positive = "HW")
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 80, mtry = 8, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1, positive = "HW")
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 70, mtry = 8, maxnodes = 40, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
pred2 <- predict(model_rf2, newdata = test)
confusionMatrix(pred2, test$result1, positive = "HW")
set.seed(2016)
model_rf2 <- randomForest(result1 ~ ., data = train, ntree = 500,
nodesize = 80, mtry = 8, maxnodes = 40, importance = T,
do.trace = T, localImp = T, replace = F) # this configuration seems to be quite good
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search = "grid")
tunegrid <- expand.grid(.mtry=c(sqrt(ncol(train))))
modellist <- list()
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search = "grid")
tunegrid <- expand.grid(.mtry=c(sqrt(ncol(train))))
modellist <- list()
for (ntree in c(1000, 1500, 2000, 2500)) {
set.seed(seed)
fit <- train(result1 ~ ., data = train, method = "rf",
metric = metric, tuneGrid = tunegrid, trControl = control, ntree = ntree)
key <- toString(ntree)
modellist[[key]] <- fit
}
for (ntree in c(1000, 1500, 2000, 2500)) {
set.seed(2016)
fit <- train(result1 ~ ., data = train, method = "rf",
metric = metric, tuneGrid = tunegrid, trControl = control, ntree = ntree)
key <- toString(ntree)
modellist[[key]] <- fit
}
for (ntree in c(1000, 1500, 2000, 2500)) {
set.seed(2016)
fit <- train(result1 ~ ., data = train, method = "rf",
tuneGrid = tunegrid, trControl = control, ntree = ntree)
key <- toString(ntree)
modellist[[key]] <- fit
}
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search = "grid")
tunegrid <- expand.grid(.mtry=c(sqrt(ncol(train))))
modellist <- list()
set.seed(2016)
fit <- train(result1 ~ ., data = train, method = "rf",
tuneGrid = tunegrid, trControl = control, ntree = 1000)
summary(soccer$result1)
set.seed(2016)
model_rf1 <- randomForest(result1 ~ ., data = train, ntree = 1000,
nodesize = 40, mtry = 5, maxnodes = 20, importance = T,
do.trace = T, localImp = T, replace = F)
library(lattice)
library(ggplot2)
library(e1071)
library(caret)
library(ROCR)
library(caTools)
library(class)
library(randomForest)
library(AUCRF)
set.seed(2016)
model_rf1 <- randomForest(result1 ~ ., data = train, ntree = 1000,
nodesize = 40, mtry = 5, maxnodes = 20, importance = T,
do.trace = T, localImp = T, replace = F)
pred1 <- predict(model_rf1, newdata = test)
pred1_prob <- predict(model_rf1, newdata = test, type = "prob") # predicted probabilities
pred1_prob
confusionMatrix(pred1, test$result1, positive = "HW") # accuracy = 0.6301
print(model_rf2)
varImpPlot(model_rf2)
varImpPlot(model_rf2, sort = T, type = 1, n.var = 40)
load("Soccer.rda")
set.seed(2016)
index <- createDataPartition(soccer$result1, p=0.8, list=F)
train <- soccer[index,]
test <- soccer[-index,]
aucrf_train <- train
aucrf_train$result1 <- factor(aucrf_train$result1, levels=c('HDW', 'HW'), labels = c(0,1))
aucrf_train$result1 <- factor(aucrf_train$result1, levels=c('HDW', 'HW'), labels = c(0,1))
model_aucrf <- AUCRF(result1 ~ ., data = aucrf_train)
set.seed(2016)
model_aucrf <- AUCRF(result1 ~ ., data = aucrf_train)
model_aucrf <- AUCRF(result1 ~ ., data = aucrf_train, ranking="MDA")
set.seed(2016)
model_aucrf <- AUCRF(result1 ~ ., data = aucrf_train, ranking="MDA")
aucrf_train <- train # to make changes to the result1 (make 0/1)
aucrf_train$result1 <- factor(aucrf_train$result1, levels=c('HDW', 'HW'), labels = c(0,1))
View(aucrf_train)
set.seed(2016)
model_aucrf <- AUCRF(result1 ~ ., data = aucrf_train, ranking="MDA")
plot(model_aucrf)
summary(model_aucrf)
model_aucrf
soccer_colnames <- colnames(soccer)
soccer_new <- soccer[, c(1:3,72)]
soccer_colnames <- colnames(soccer)
soccer_new <- soccer[, c(1:3,72)]
soccer_colnames <- colnames(soccer)
i_home = 4
i_away = 38
soccer_new <- soccer[, c(1:3,72)]
soccer_columns <- colnames(soccer)
i_home = 4
i_away = 38
repeat {
if (i_home <= 37)
{
home_cols <- soccer_columns[i_home]
away_cols <- soccer_columns[i_away]
columns_final <- substr(home_cols, 6, nchar(homeColname))
soccer_new[[columns_final]] <- (soccer[[home_cols]] + soccer[[away_cols]])/2
i_home = i_home + 1 # no increment operator :(
i_away = i_away + 1
}
else{break}
}
repeat {
if (i_home <= 37)
{
home_cols <- soccer_columns[i_home]
away_cols <- soccer_columns[i_away]
columns_final <- substr(home_cols, 6, nchar(home_cols))
soccer_new[[columns_final]] <- (soccer[[home_cols]] + soccer[[away_cols]])/2
i_home = i_home + 1 # no increment operator :(
i_away = i_away + 1
}
else{break}
}
set.seed(2016)
set.seed(2016)
index <- createDataPartition(soccer_new$result1, p = 0.8, list = FALSE)
train_final <- new_soccer[index,]
test_final <- new_soccer[-index,]
set.seed(2016)
index <- createDataPartition(soccer_new$result1, p = 0.8, list = FALSE)
train_final <- soccer_new[index,]
test_final <- soccer_new[-index,]
set.seed(2016)
model_rf_final <- randomForest(result1 ~ ., data = train_final, ntree = 500,
nodesize = 80, mtry = 8, maxnodes = 40, importance = T,
do.trace = T, localImp = T, replace = F)
pred_final <- predict(model_rf_final, newdata = test_final)
confusionMatrix(pred_final, test_final$result1, positive = "HW")
pairs(home_balance~home_strength, data=soccer)
pairs(soccer)
qplot(away_sliding_tackle, data=soccer, binwidth=0.8)
pairs(home_balance~home_strength, data=soccer)
pairs(home_balance ~ home_strength, data = soccer) # balance/strength correlation for home
pairs(away_balance ~ away_strength, data = soccer) # balance/strength correlation for away
qplot(away_sliding_tackle, data=soccer, binwidth=0.8)
View(soccer)
View(soccer)
qplot(home_potential, data = soccer, binwidth = 0.8)
cor(soccer[, 1:35])
cor(soccer[, 1:35])
cor(soccer[, 36:71])
pairs(soccer[, 1:35])
pairs(soccer[, 1:20])
qplot(home_heading_accuracy, data = soccer, binwidth = 0.8)
library(lattice)
library(ggplot2)
library(e1071)
library(caret)
library(ROCR)
library(caTools)
library(class)
library(randomForest)
library(AUCRF)
load("Soccer.rda")
set.seed(2016)
index <- createDataPartition(soccer$result1, p=0.8, list=F)
train <- soccer[index,]
test <- soccer[-index,]
pairs(home_balance ~ home_strength, data = soccer) # balance/strength correlation for home
pairs(away_balance ~ away_strength, data = soccer) # balance/strength correlation for away
qplot(home_potential, data = soccer, binwidth = 0.8) # the concentration is between 70 and 80, pretty high numbers
qplot(home_heading_accuracy, data = soccer, binwidth = 0.8) #
qplot(away_heading_accuracy, data = soccer, binwidth = 0.8)
qplot(home_heading_accuracy, data = soccer, binwidth = 0.8) # the concentration is between 55 and 65
qplot(away_heading_accuracy, data = soccer, binwidth = 0.8) # the concentration is between 55 and 65
