cp=0.01)
prp(fit)
fit
fancyRpartPlot(fit)
library(rpart)
library(rpart.plot)
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rattle")
library(rpart)
library(rpart.plot)
library(rattle)
library(caret)
library(ROCR)
Titanic<-read.csv("Titanic_imputed.csv", nrows=1310)
Titanic$pclass<-factor(Titanic$pclass, levels=c(1,2,3), labels=c("1st", "2nd", "3rd"))
Titanic$survived<-factor(Titanic$survived, levels=c(0,1), labels=c("No", "Yes"))
###Create training and testing datasets
set.seed(2015)
trainIndex <- createDataPartition(Titanic$survived, p = .75, list = FALSE)
Train<-Titanic[trainIndex,]
Test<-Titanic[-trainIndex,]
###contingency table and conditional probabilities
#The decision tree with only gender as an independent variable
fit<-rpart(survived~sex, data=Titanic, method="class")
prp(fit,
type=1, #type of the plot
extra=4, #display extra information
faclen=5,# lenght of the factor variable to be shown
main="Decision tree for titanic")
# from rattle
fancyRpartPlot(fit)
###Lets have a look at the contingency table
prop.table(table(Titanic$sex,Titanic$survived),1)
##Lets try with the pclass
fit<-rpart(survived~pclass, data=Titanic, method="class")
prp(fit, type=1, extra=4, faclen=5, main="Decision tree for titanic")
prop.table(table(Titanic$pclass,Titanic$survived),1)
fit<-rpart(survived~sex+age+sibsp+parch+pclass,
data=Train, method="class", minsplit=25)
prp(fit, type=1, extra=1, faclen=0, tweak=0.5) #lenght of factor level names
#tweak text size
fancyRpartPlot(fit, tweak=1.1)
asRules(fit) #rattle
###Predict the class
pred_class<-predict(fit, Test, type="class")
###Confusion Matrix, Lets assume that "Yes" is our positive instance
table(Actual=Test$survived, Predicted=pred_class)
######Do it yourself#####
weather<-read.csv("weather.csv")
summary(weather)
###testing and training datasets
set.seed(1982)
trainIndex <- createDataPartition(weather$RainToday, p = .75, list = FALSE)
Train<-weather[trainIndex,]
Test<-weather[-trainIndex,]
fit<-rpart(RainTomorrow~.-Rainfall, data=Train, method="class")
prp(fit, type=1, extra=4)
asRules(fit) #rattle
#printcp(fit)
fancyRpartPlot(fit, tweak=1.2)
##Making predictions##
pred_class<-predict(fit, Test, type="class")
###Confusion Matrix, Lets assume that "Yes" is our positive instance
table(Actual=Test$RainToday, Predicted=pred_class)
### Create confusion matrix
confusionMatrix(pred_class, Test$RainToday, positive = "Yes")
# ROC curve
pred_prob <- predict(fit, Test, type="prob")
pred <- prediction(pred_prob[,2], Test$RainTomorrow)
perf <- performance(pred,"tpr","fpr")
performance(pred, "auc")@y.values
plot(perf)
# tunning the rpart tree
set.seed(2015)
fit<-rpart(RainTomorrow~.-Rainfall, data=Train,
method="class",
#           minsplit=25, # minimum number of cases to be in the node
# minbucket=15 # the minimum number of observations in any terminal <leaf> node
cp=0.01)
prp(fit)
fit
fancyRpartPlot(fit)
load(voting_train.rda)
load("voting_train.rda")
load("voting_test.rda")
library(e1071)
model <- naiveBayes(Class~., data = Train, laplace = 1)
names(model)
summary(model)
model$apriori
model$tables
model$levels
model$call
pred <- predict(model, newdata = Test)
library(caret)
confusionMatrix(pred, Test$Class, )
confusionMatrix(pred, Test$Class)
View(Train)
pred_test <- predict(model, newdata = Test)
confusionMatrix(pred, Test$Class) # confusion matrix
rm(pred)
pred_test_prob <- predict(model, newdata = Test, type = "raw")
pred_test_prob
View(pred_test_prob)
hello world
"hello world"
pred_t <- pred_test_prob
library(ROCR)
p_test <- prediction(pred_test_prob[, 1], Test$Class, label.ordering = c("republican", "democrat"))
rm(pred_t)
p_test2 <- prediction(pred_test_prob[, 2], Test$Class, label.ordering = c("republican", "democrat"))
p_test1 <- prediction(pred_test_prob[, 1], Test$Class, label.ordering = c("republican", "democrat"))
rm(p_test)
p_test1
perf <- performance(p_test1, "tpr", "fpr")
plot(perf)
performance(p_test1, "auc")@y.values
perf <- performance(p_test1, "tpr", "fpr")
plot(perf)
performance(p_test1, "auc")@y.values
perf <- performance(p_test2, "tpr", "fpr")
plot(perf)
performance(p_test2, "auc")@y.values
p_test1 <- prediction(pred_test_prob[, 1], Test$Class, label.ordering = c("republican", "democrat"))
p_test2 <- prediction(pred_test_prob[, 2], Test$Class, label.ordering = c("democrat", "republican"))
perf <- performance(p_test1, "tpr", "fpr")
plot(perf)
performance(p_test1, "auc")@y.values
# for positive = republican
perf <- performance(p_test2, "tpr", "fpr")
plot(perf)
performance(p_test2, "auc")@y.values
perf <- performance(p_test1, "tpr", "fpr")
plot(perf)
performance(p_test1, "auc")@y.values
perf <- performance(p_test2, "tpr", "fpr")
plot(perf)
performance(p_test2, "auc")@y.values
spam <- read.csv("spam.csv")
is_spam <- spam$is_spam
spam <- read.csv("spam.csv")
is_spam <- spam$is_spam
View(spam)
library(caTools)
index <- createDataPartition(spam$is_spam, p = .8, list = FALSE)
Train <- Titanic[index,]
Test <- Titanic[-index,]
index <- createDataPartition(spam$is_spam, p = .8, list = FALSE)
Train <- spam[index,]
Test <- spam[-index,]
model_spam <- naiveBayes(is_spam~., data = Train, laplace = 1)
pred_test_spam <- predict(model_spam, newdata = Test)
confusionMatrix(pred_test_spam, Test$Class)
confusionMatrix(pred_test_spam, Test$is_spam)
pred_test_spam <- predict(model_spam, newdata = Test)
confusionMatrix(pred_test_spam, Test$is_spam)
set.seed(2016)
index <- createDataPartition(spam$is_spam, p = .8, list = FALSE)
Train <- spam[index,]
Test <- spam[-index,]
model_spam <- naiveBayes(is_spam~., data = Train, laplace = 1)
pred_test_spam <- predict(model_spam, newdata = Test)
confusionMatrix(pred_test_spam, Test$is_spam)
names(model)
names(model_spam)
pred_test_spam <- predict(model, newdata = Test)
pred_test_spam <- predict(model_spam, newdata = Test)
confusionMatrix(pred_test, Test$is_spam) # confusion matrix
confusionMatrix(pred_test_spam, Test$is_spam) # confusion matrix
confusionMatrix(pred_test_spam, Test$is_spam, positive = "Yes") # confusion matrix
spam$is_spam <- factor(spam$is_spam, levels = c(0, 1), labels = c("No", "Yes"))
spam <- read.csv("spam.csv")
spam$is_spam <- factor(spam$is_spam, levels = c(0, 1), labels = c("No", "Yes"))
is_spam <- spam$is_spam
set.seed(2016)
spam <- read.csv("spam.csv")
spam$is_spam <- factor(spam$is_spam, levels = c(0, 1), labels = c("No", "Yes"))
is_spam <- spam$is_spam
set.seed(2016)
index <- createDataPartition(spam$is_spam, p = .8, list = FALSE)
Train <- spam[index,]
Test <- spam[-index,]
model_spam <- naiveBayes(is_spam~., data = Train, laplace = 1)
pred_test_spam <- predict(model_spam, newdata = Test)
confusionMatrix(pred_test_spam, Test$is_spam, positive = "Yes")
pred_test_prob_spam <- predict(model_spam, newdata = Test, type = "raw")
p_test_spam <- prediction(pred_test_prob_spam[, 1], Test$Class, label.ordering = c("Yes", "No"))
p_test_spam <- prediction(pred_test_prob_spam[, 1], Test$is_spam, label.ordering = c("Yes", "No"))
perf <- performance(p_test_spam, "tpr", "fpr")
plot(perf)
performance(p_test_spam, "auc")@y.values
Train <- read.csv("cancer_train")
Test <- read.csv("cancer_test")
Train <- read.csv("cancer_train.csv")
Test <- read.csv("cancer_test.csv")
library(e1071)
library(caret)
library(caTools)
library(ROCR)
View(Test)
model <- naiveBayes(Class~., data = Train)
pred <- predict(model, newdata = Test, type = "raw")
data("iris")
data(iris)
data("iris")
data(iris)
View(iris)
View(iris)
View(iris)
iris1 <- iris[if(Species != "virginica"),]
?subset
iris1 <- subset(iris, if(Species != "virginica"))
iris1 <- subset(iris, subset = iris[Species != "virginica")
iris1 <- subset(iris, subset = iris[Species != "virginica"])
iris1 <- iris[iris$Species != 'virginica', ]
View(iris1)
table(iris1$Species)
iris1$Species <- factor(iris1$Species)
table(iris1$Species)
library(ggplot2)
install.packages("ggplot2"")
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
library(MASS)
install.packages("MASS")
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 4, slope = 2, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 4, slope = 2, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 4, slope = 1, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2, slope = 1, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2, slope = 0.8, size = 3)
geom_abline(intercept = 2.5, slope = 0.8, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.5, slope = 0.8, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.8, slope = 0.8, size = 3)
geom_abline(intercept = 2.7, slope = 0.8, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.9, slope = 0.8, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.8, slope = 0.8, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.8, slope = 0.9, size = 3)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.8, slope = 0.8, size = 3)
iris2 <- iris[iris$Species == 'versicolor', ]
View(iris2)
iris2 <- iris[iris$Species == 'versicolor', ]
qplot(Sepal.Width, Sepal.Length, data = iris2, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.8, slope = 0.8, size = 3)
iris2 <- iris[iris$Species != 'versicolor', ]
qplot(Sepal.Width, Sepal.Length, data = iris2, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.8, slope = 0.8, size = 3)
iris2 <- iris[iris$Species != 'versicolor', ]
iris2$Species <- factor(iris2$Species)
qplot(Sepal.Width, Sepal.Length, data = iris2, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.8, slope = 0.8, size = 3)
wine <- read.csv("wine.csv")
View(wine)
install.packages('klaR')
library(ggplot2)
library(MASS)
library(klaR)
library(caret)
install.packages("caret")
library(caret)
index <- createDataPartition(data = wine, p = 0.7, list = FALSE)
index <- createDataPartition(wine, p = 0.7, list = FALSE)
index <- createDataPartition(wine$Type~., p = 0.7, list = FALSE)
index <- createDataPartition(wine$Type~., p = 0.7, list = FALSE)
index <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
train <- [index,]
test <- [-index,]
train <- wine[index,]
test <- wine[-index,]
model <- lda(Type~., data = train)
model
X <- predict(model)
# Wine prediction
wine <- read.csv("wine.csv")
index <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
train <- wine[index,]
test <- wine[-index,]
model <- lda(Type~., data = train)
model
pred <- predict(model, newdata = test)
qplot(pred$x[,1], X$x[,2], colour = wine$Type)
qplot(pred$x[,1], X$x[,2], colour = wine$Type)
qplot(pred$x[,1], pred$x[,2], colour = wine$Type)
qplot(pred$x[,1], pred$x[,2], colour = wine$Type)
qplot(x = x[,1], y = x[,2], data = test, colour = Type)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +   geom_abline(intercept = 2.8, slope = 0.8, size = 3)
geom_abline(intercept = 2.8, slope = 0.8, size = 1)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(intercept = 2.8, slope = 0.8, size = 1)
model
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(intercept = 2.8, slope = 0.8, size = 1) +
geom_abline(intercept = 2.8, slope = 0.8, size = 1)
geom_abline(intercept = 2.8, slope = 0.7329, size = 1) +
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(intercept = 2.8, slope = 0.7329, size = 1) +
geom_abline(intercept = 2.8, slope = 0.2671, size = 1)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(intercept = 2.8, slope = 0.7329, size = 1) +
geom_abline(intercept = 2.8, slope = 0.2671, size = 1)
library(ggplot2)
library(MASS)
library(klaR)
library(caret)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(intercept = 2.8, slope = 0.7329, size = 1) +
geom_abline(intercept = 2.8, slope = 0.2671, size = 1)
wine$Type <- factor(wine$Type)
wine <- read.csv("wine.csv")
wine$Type <- factor(wine$Type)
index <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
train <- wine[index,]
test <- wine[-index,]
model <- lda(Type~., data = train)
model
pred <- predict(model, newdata = test)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(intercept = 2.8, slope = 0.7329, size = 1) +
geom_abline(intercept = 2.8, slope = 0.2671, size = 1)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(slope = 0.7329, size = 1) +
geom_abline(slope = 0.2671, size = 1)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(0, slope = 0.7329, size = 1) +
geom_abline(slope = 0.2671, size = 1)
table(wine$Type, pred$class)
pred <- predict(model, newdata = test)
table(wine$Type, pred$class)
library(ggplot2)
library(MASS)
library(klaR)
library(caret)
data(iris)
iris1 <- iris[iris$Species != 'virginica', ]
table(iris1$Species)
iris1$Species <- factor(iris1$Species)
qplot(Sepal.Width, Sepal.Length, data = iris1, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.8, slope = 0.8, size = 3)
iris2 <- iris[iris$Species != 'versicolor', ]
iris2$Species <- factor(iris2$Species)
qplot(Sepal.Width, Sepal.Length, data = iris2, colour = Species,
main = "Scatterplot of Sepal Length and Sepal Width")+
geom_abline(intercept = 2.8, slope = 0.8, size = 3)
model <- lda(Species~., data = iris)
model
X <- predict(model)
table (iris$Species, X$class)
qplot(X$x[,1], X$x[,2], colour = iris$Species)
ldahist(X$x[,1], iris$Species)
ldahist(X$x[,2], iris$Species)
partimat(Species~., data = iris, method = 'lda')
## Wine prediction
wine <- read.csv("wine.csv")
wine$Type <- factor(wine$Type)
index <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
train <- wine[index,]
test <- wine[-index,]
model <- lda(Type~., data = train)
model
pred <- predict(model, newdata = test)
table(wine$Type, pred$class)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(slope = 0.7329, size = 1) +
geom_abline(slope = 0.2671, size = 1)
table (iris$Species, X$class)
wine <- read.csv("wine.csv")
wine$Type <- factor(wine$Type)
set.seed(1973)
index <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
train <- wine[index,]
test <- wine[-index,]
model <- lda(Type~., data = train)
model
pred <- predict(model, newdata = test)
table(wine$Type, pred$class)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(slope = 0.7329, size = 1) +
geom_abline(slope = 0.2671, size = 1)
set.seed(1973)
index <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
train <- wine[index,]
test <- wine[-index,]
model <- lda(Type~., data = train)
model
pred <- predict(model, newdata = test)
qplot(pred$x[,1], pred$x[,2], colour = test$Type) +
geom_abline(slope = 0.6589, size = 1) +
geom_abline(slope = 0.3411, size = 1)
data("iris")
install.packages("e1071")
library(e1071)
data("iris")
iris1 <- iris[iris$Species != 'virginica', ]
iris1$Species <- factor(iris1$Species)
M <- svm(Species~Sepal.Width+Sepal.Length, data = iris1, kernel = "linear")
plot(M, data = iris1, Sepal.Length~Sepal.Width)
M <- svm(Species~Sepal.Width + Sepal.Length, data = iris1, kernel = "linear")
plot(M, data = iris1, Sepal.Length~Sepal.Width)
library(e1071)
data("iris")
iris1 <- iris[iris$Species != 'virginica', ]
iris1$Species <- factor(iris1$Species)
svm_model1 <- svm(Species~Sepal.Width + Sepal.Length, data = iris1, kernel = "linear")
plot(svm_model1, data = iris1, Sepal.Length~Sepal.Width)
svm_model2 <- svm(Species~Sepal.Width + Sepal.Length, data = iris1, kernel = "polinomial")
plot(svm_model2, data = iris1, Sepal.Length~Sepal.Width)
svm_model2 <- svm(Species~Sepal.Width + Sepal.Length, data = iris1, kernel = "polinomial")
plot(svm_model2, data = iris1, Sepal.Length~Sepal.Width)
svm_model2 <- svm(Species~Sepal.Width + Sepal.Length, data = iris1, kernel = "polynomial")
plot(svm_model2, data = iris1, Sepal.Length~Sepal.Width)
svm_model2 <- svm(Species~Sepal.Width + Sepal.Length,
data = iris1, kernel = "polynomial", probability = TRUE)
pred <- predict(svm_model2, data = iris)
pred <- predict(svm_model2, data = iris1)
pred <- predict(svm_model2, data = iris1, probability = TRUE)
library(e1071)
data("iris")
iris1 <- iris[iris$Species != 'virginica', ]
iris1$Species <- factor(iris1$Species)
svm_model1 <- svm(Species~Sepal.Width + Sepal.Length,
data = iris1, kernel = "linear")
plot(svm_model1, data = iris1, Sepal.Length~Sepal.Width)
svm_model2 <- svm(Species~Sepal.Width + Sepal.Length,
data = iris1, kernel = "polynomial", probability = TRUE)
pred <- predict(svm_model2, data = iris1, probability = TRUE)
plot(svm_model2, data = iris1, Sepal.Length~Sepal.Width)
pred
pred <- predict(svm_model2, data = iris1, probability = TRUE)
pred
install.packages("neuralnet")
library(neuralnet)
data(infert)
data(infert)
View(infert)
View(infert)
View(infert)
fm <- formula(case~age+parity+induced+spontaneous)
names(infert)
class(fm)
set.seed(2016)
model_nn <- neuralnet(fm, data = infert,
hidden = 2, linear.output = F,
err.fct = "ce", rep = 4)
set.seed(2016)
model_nn <- neuralnet(fm, data = infert,
hidden = 2, linear.output = FALSE,
err.fct = "ce", rep = 4)
summary(model_nn)
plot(model_nn)
model_nn$result.matrix
set.seed(2016)
model_nn <- neuralnet(fm, data = infert,
hidden = 2, linear.output = FALSE,
err.fct = "ce", rep = 4)
set.seed(2016)
model_nn <- neuralnet(fm, data = infert,
hidden = c(2, 2), linear.output = FALSE,
err.fct = "ce", rep = 4)
set.seed(2016)
model_nn <- neuralnet(fm, data = infert,
hidden = c(2, 2), linear.output = FALSE,
err.fct = "ce", rep = 4)
summary(model_nn)
plot(model_nn)
set.seed(2016)
model_nn <- neuralnet(fm, data = infert,
hidden = c(2, 2), linear.output = FALSE,
err.fct = "ce", rep = 4)
summary(model_nn)
plot(model_nn)
model_nn$result.matrix # weights
library(neuralnet)
data(infert)
names(infert)
fm <- formula(case ~ age + parity + induced + spontaneous)
set.seed(2016)
model_nn <- neuralnet(fm, data = infert,
hidden = c(2, 2), linear.output = FALSE,
err.fct = "ce", rep = 4)
summary(model_nn)
plot(model_nn)
model_nn$result.matrix # weights
data <- read.csv("Diabetes.csv")
